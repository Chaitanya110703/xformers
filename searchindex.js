Search.setIndex({"docnames": ["components/attentions", "components/feedforward", "components/index", "components/mha", "components/ops", "components/position_embedding", "components/reversible", "custom_parts/index", "factory/block", "factory/index", "factory/model", "index", "tutorials/blocksparse", "tutorials/extend_attentions", "tutorials/hierarchical", "tutorials/index", "tutorials/pytorch_encoder", "tutorials/reversible", "tutorials/sparse_vit", "tutorials/triton", "tutorials/use_attention", "what_is_xformers"], "filenames": ["components/attentions.rst", "components/feedforward.rst", "components/index.rst", "components/mha.rst", "components/ops.rst", "components/position_embedding.rst", "components/reversible.rst", "custom_parts/index.rst", "factory/block.rst", "factory/index.rst", "factory/model.rst", "index.rst", "tutorials/blocksparse.rst", "tutorials/extend_attentions.rst", "tutorials/hierarchical.rst", "tutorials/index.rst", "tutorials/pytorch_encoder.rst", "tutorials/reversible.rst", "tutorials/sparse_vit.rst", "tutorials/triton.rst", "tutorials/use_attention.rst", "what_is_xformers.rst"], "titles": ["Attention mechanisms", "Feedforward mechanisms", "API Reference", "Multi Head Attention", "xFormers optimized operators", "Position Embeddings", "Reversible layer", "Custom parts reference", "Block factory", "Factory", "Model factory", "Welcome to xFormers\u2019s documentation!", "Using BlockSparseAttention", "Extend the xFormers parts zoo", "Hierarchical Transformers", "Tutorials", "Building an encoder, comparing to PyTorch", "Using the Reversible block", "Replace all attentions from an existing ViT model with a sparse equivalent?", "Using Triton-based layers", "I\u2019m only interested in testing out the attention mechanisms that are hosted here", "What is xFormers?"], "terms": {"class": [0, 1, 3, 4, 5, 6, 8, 10, 13, 17], "xformer": [0, 1, 2, 3, 5, 6, 7, 8, 10, 12, 14, 15, 17, 18, 19, 20], "compon": [0, 1, 3, 5, 6, 12, 13, 17, 18, 20], "scaleddotproduct": [0, 18], "dropout": [0, 1, 3, 4, 5, 10, 12, 13, 16, 20], "float": [0, 1, 3, 4, 5, 13, 16], "0": [0, 3, 4, 5, 7, 12, 16, 20], "causal": [0, 4, 10, 12, 13, 16], "bool": [0, 1, 3, 5, 6, 8, 10, 13, 17], "fals": [0, 3, 6, 10, 13, 14, 16, 17, 18, 19], "seq_len": [0, 5, 10, 12, 14, 16, 20], "option": [0, 1, 3, 4, 8, 9, 10, 11, 13, 16, 17], "int": [0, 1, 3, 4, 5, 6, 13, 16, 17], "none": [0, 1, 3, 4, 8, 10, 13, 16, 19], "to_seq_len": 0, "arg": [0, 1, 3, 5, 6, 13], "kwarg": [0, 1, 3, 5, 6, 8, 12, 13, 17], "sourc": [0, 1, 3, 4, 5, 6, 7, 8, 10, 18, 21], "base": [0, 1, 4, 5, 7, 8, 10, 11, 13, 15, 18, 20], "implement": [0, 5, 7, 8, 16, 17], "scale": [0, 4, 7, 16], "dot": [0, 7, 16], "product": [0, 7, 16], "propos": [0, 3, 10, 14, 16, 17, 18, 19, 21], "all": [0, 3, 4, 11, 13, 15, 17, 19, 20, 21], "you": [0, 3, 7, 12, 13, 14, 16, 17, 18, 19, 20, 21], "need": [0, 3, 4, 7, 12, 13, 16, 17], "vaswani": [0, 3, 16, 17], "et": [0, 3, 5, 10, 16, 17], "al": [0, 3, 5, 10, 16, 17], "mask": [0, 3, 4, 7, 12, 16, 18, 20], "attentionmask": [0, 8], "forward": [0, 1, 3, 4, 5, 6, 8, 10, 12, 13, 16, 17], "q": [0, 3, 4, 5, 13], "tensor": [0, 1, 3, 4, 5, 6, 8, 10, 13, 16, 17, 18, 19], "k": [0, 3, 4, 5, 13], "v": [0, 3, 4, 13], "att_mask": [0, 3, 8, 12, 18], "union": [0, 1, 4, 5, 8, 10, 17], "A": [0, 3, 4, 5, 7, 8, 10, 12, 14, 16, 17], "2d": 0, "3d": 0, "which": [0, 1, 4, 5, 7, 11, 12, 13, 14, 16, 17, 18, 19, 20, 21], "ignor": 0, "certain": 0, "posit": [0, 2, 10, 11, 16], "If": [0, 4, 12, 16], "boolean": [0, 17], "valu": [0, 3, 4, 7, 12, 16, 21], "true": [0, 1, 3, 6, 7, 10, 12, 13, 16, 17, 19], "keep": [0, 16], "while": [0, 16], "kei": [0, 1, 3, 4, 5, 12], "pad": [0, 4, 14], "dimens": [0, 3, 4, 12, 14, 16, 19, 20], "batch": [0, 3, 4, 12, 13, 16, 20], "x": [0, 4, 5, 6, 7, 8, 13, 16, 17, 19, 20], "sequenc": [0, 3, 4, 12, 14, 16, 17, 19, 20], "length": [0, 3, 4, 12], "OR": 0, "can": [0, 3, 4, 7, 10, 11, 12, 13, 16, 17, 18, 19, 20, 21], "combin": [0, 11, 17, 20], "pass": [0, 4, 12, 17], "here": [0, 11, 12, 15, 16], "method": [0, 5], "maybe_merge_mask": 0, "provid": [0, 3, 7, 12, 14, 19, 20], "util": 0, "us": [0, 1, 3, 4, 5, 7, 10, 11, 13, 15, 16, 18, 20], "merg": 0, "ha": [0, 4, 16], "type": [0, 1, 4, 5, 13, 16], "an": [0, 4, 5, 11, 12, 14, 15, 19, 20], "addit": [0, 4], "expect": [0, 3, 16, 19, 20], "ar": [0, 3, 4, 5, 7, 9, 11, 13, 14, 15, 16, 17, 18, 19, 21], "inf": [0, 4], "localattent": [0, 16], "window_s": 0, "5": [0, 4], "force_spars": 0, "__init__": [0, 10, 13, 17], "slide": 0, "window": 0, "routingtransform": 0, "longform": 0, "bigbird": 0, "paramet": [0, 3, 4, 16, 18, 20], "probabl": [0, 4, 12, 16], "output": [0, 3, 4, 10, 17, 18], "randomli": 0, "drop": [0, 12, 16, 19], "train": [0, 1, 3, 5, 6, 8, 10, 16], "time": [0, 12, 16, 19], "appli": [0, 4, 13, 17], "cannot": [0, 4, 16, 21], "futur": [0, 16], "overal": 0, "size": [0, 3, 4, 12], "local": [0, 13, 16, 19, 21], "odd": 0, "number": [0, 3, 4, 14, 17, 19], "evenli": 0, "distribut": [0, 17], "both": [0, 13], "side": 0, "each": [0, 4, 19, 21], "queri": [0, 3, 4, 5, 12], "linformerattent": 0, "linform": [0, 16], "from": [0, 1, 4, 5, 7, 11, 12, 13, 14, 15, 16, 17, 19, 20], "self": [0, 1, 3, 4, 12, 13, 17], "linear": [0, 4, 16], "complex": 0, "wang": 0, "2020": [0, 17], "The": [0, 3, 4, 5, 7, 10, 12, 13, 14, 16, 17, 18, 19, 20], "origin": [0, 14, 17], "notat": 0, "kept": 0, "nystromattent": [0, 13], "num_head": [0, 3, 10, 12, 13, 16, 18, 20], "num_landmark": [0, 13], "64": [0, 13, 14, 16], "landmark_pool": [0, 13], "modul": [0, 1, 3, 5, 6, 7, 8, 10, 13, 17, 18], "use_razavi_pinvers": [0, 13], "pinverse_original_init": [0, 13], "inv_iter": [0, 13], "6": [0, 4, 7, 12, 13, 16], "v_skip_connect": [0, 13], "conv_kernel_s": [0, 13], "nystrom": [0, 13, 16], "nystromform": 0, "algorithm": 0, "approxim": 0, "xiong": 0, "y": [0, 4, 6, 16, 19], "zeng": 0, "z": 0, "chakraborti": 0, "r": [0, 7, 17], "tan": 0, "m": [0, 4, 11, 15, 17], "fung": 0, "g": [0, 6, 17], "li": 0, "singh": 0, "2021": [0, 19], "refer": [0, 11, 18, 21], "codebas": 0, "http": [0, 19], "github": [0, 7], "com": [0, 7], "mlpen": 0, "key_padding_mask": [0, 3], "onli": [0, 4, 7, 10, 11, 15, 19], "accept": [0, 13], "must": [0, 4], "1": [0, 4, 5, 6, 12, 14, 16, 17, 18, 19, 20], "correct": 0, "mean": [0, 16, 17, 21], "randomattent": [0, 16], "01": 0, "constant_mask": 0, "random": [0, 12], "instanc": [0, 1, 5, 10, 13], "case": [0, 4, 16, 18, 19, 20], "attend": [0, 4, 16], "set": [0, 4, 7, 16, 17], "thi": [0, 1, 3, 4, 5, 8, 10, 12, 13, 14, 16, 17, 18, 19, 20, 21], "spars": [0, 11, 12, 15], "awar": [0, 3, 18, 19], "empti": [0, 4, 12], "part": [0, 1, 4, 5, 15, 16], "repres": 0, "memori": [0, 2, 8, 12, 16, 17], "ratio": 0, "same": [0, 4, 13, 16, 17], "orthoformerattent": 0, "32": [0, 4, 12, 14], "subsample_fract": 0, "landmark_select": 0, "landmarkselect": 0, "orthogon": 0, "orthoform": 0, "your": [0, 13, 16, 17, 20], "ey": 0, "ball": 0, "trajectori": 0, "video": 0, "transform": [0, 5, 8, 10, 11, 15, 18, 19, 21], "patrick": 0, "campbel": 0, "d": [0, 4], "asano": 0, "misra": 0, "i": [0, 4, 11, 15], "metz": 0, "f": [0, 4, 6, 12, 17], "feichtenhof": 0, "c": [0, 13], "vedaldi": 0, "henriqu": 0, "j": 0, "facebookresearch": 0, "motionform": 0, "globalattent": 0, "attention_query_mask": [0, 20], "_": [0, 5], "__": [0, 5], "global": 0, "label": 0, "other": [0, 5, 13, 16, 17, 18, 19, 21], "neg": [0, 4], "ani": [0, 1, 4, 5, 7, 10, 13, 18, 19, 21], "element": [0, 14, 16], "zero": [0, 4], "torch": [0, 4, 12, 13, 16, 17, 18, 19, 20], "favorattent": 0, "dim_featur": 0, "dim_head": [0, 14], "iter_before_redraw": 0, "feature_map_typ": 0, "featuremaptyp": 0, "smreg": 0, "normalize_input": 0, "kernel": [0, 4, 11, 19], "perform": [0, 4, 19], "rethink": 0, "choromanski": 0, "favor": [0, 16], "stand": 0, "fast": [0, 4, 12, 21], "via": [0, 4, 17], "featur": [0, 16], "space": [0, 4], "step": [0, 16], "call": [0, 1, 4, 5, 13], "befor": [0, 4], "redraw": 0, "map": 0, "being": [0, 3, 12, 19, 21], "typic": [0, 14, 18, 19], "sub": 0, "multi": [0, 2, 4, 11, 17, 20], "head": [0, 2, 4, 11, 12, 16, 17, 20], "classmethod": [0, 1, 3, 4, 8, 10], "from_config": [0, 1, 3, 5, 8, 10, 16], "config": [0, 1, 3, 5, 8, 10, 13, 14, 16], "attentionconfig": [0, 13], "abstract": 0, "additive_mask": 0, "is_caus": 0, "object": [0, 4, 10, 16], "hold": 0, "along": [0, 4, 12, 14, 16, 19], "coupl": [0, 7, 13, 16, 19], "helper": [0, 10, 12, 14, 17, 18, 20], "attribut": 0, "to_bool": 0, "from_bool": 0, "creat": [0, 4, 7, 11], "given": [0, 1, 4, 5, 10, 13, 18, 20, 21], "pattern": [0, 12, 18], "warn": [0, 7], "we": [0, 4, 7, 12, 13, 14, 16, 17, 18, 20], "assum": [0, 1, 4, 5, 16], "impli": 0, "should": [0, 4, 16, 17, 20], "comput": [0, 4, 7, 12, 17], "from_multipl": 0, "multipl": [0, 16, 17], "make_caus": [0, 4], "devic": [0, 4, 12, 16, 20], "dtype": [0, 4, 12, 16], "make_crop": 0, "return": [0, 4, 16, 18], "crop": 0, "whose": 0, "underli": 0, "view": 0, "one": [0, 4, 12, 13, 14, 16, 17, 18, 21], "properti": 0, "is_spars": 0, "ndim": 0, "shape": [0, 4, 16, 18], "build_attent": [0, 20], "dict": [0, 1, 5, 10, 20], "str": [0, 1, 4, 5, 10], "build": [0, 1, 5, 12, 15, 17, 20, 21], "name": [0, 1, 5, 7, 10, 13, 16, 18, 20], "determin": [0, 1, 5], "what": [0, 1, 5, 16], "instanti": [0, 1, 5, 16, 20], "For": [0, 1, 4, 5, 16], "my_attent": 0, "foo": [0, 1, 5], "bar": [0, 1, 5], "find": [0, 1, 5], "wa": [0, 1, 5, 13, 17, 20], "regist": [0, 1, 3, 5, 13], "see": [0, 1, 4, 5, 7, 16, 17], "register_attent": [0, 13], "subclass": [0, 1, 5], "decor": [0, 1, 5], "allow": [0, 1, 4, 5, 20, 21], "configur": [0, 1, 5, 10, 13, 14], "file": [0, 1, 5, 20], "even": [0, 1, 5, 13], "itself": [0, 1, 5], "librari": [0, 1, 3, 5, 11, 16, 21], "mlp": [1, 10, 14, 16, 17], "dim_model": [1, 3, 5, 10, 12, 16, 20], "activ": [1, 7, 10, 16, 17], "hidden_layer_multipli": [1, 10, 16], "bia": [1, 3, 4, 16], "input": [1, 3, 4, 16, 17, 20], "feedforwardconfig": 1, "build_feedforward": 1, "attent": [1, 2, 5, 7, 10, 11, 12, 13, 15, 16, 17], "my_feedforward": 1, "register_feedforward": 1, "optim": [2, 11, 19, 21], "oper": [2, 11, 19], "effici": [2, 14, 17], "mechan": [2, 3, 4, 11, 13, 15, 16, 18], "feedforward": [2, 11, 14, 16, 17], "embed": [2, 3, 4, 10, 11, 14, 16], "revers": [2, 8, 10, 11, 15, 16], "layer": [2, 10, 11, 14, 15, 17], "multiheaddispatch": [3, 12, 16, 20], "tupl": [3, 4, 5, 8], "residual_dropout": [3, 10, 12, 16, 20], "use_separate_proj_weight": 3, "dim_kei": 3, "dim_valu": 3, "in_proj_contain": [3, 16], "inputproject": [3, 16], "use_rotary_embed": [3, 10, 14], "out_proj": 3, "dispatch": [3, 4, 20], "project": 3, "end": 3, "follow": [3, 4, 7, 10, 13, 14, 16, 17, 18, 19, 21], "architectur": [3, 13, 14, 16, 20, 21], "actual": [3, 12, 19], "vari": [3, 4], "well": [3, 18], "wrap": [3, 17], "make": [3, 4, 12, 14, 16, 17, 19, 20], "them": [3, 13, 17, 19], "model": [3, 7, 9, 14, 15, 17, 20, 21], "whether": [3, 17, 20], "amount": 3, "residu": [3, 16, 17], "path": [3, 13, 16, 17, 20], "differ": [3, 4, 12, 13, 16, 17], "weight": [3, 10, 18], "rotari": [3, 5], "emb": [3, 10, 12, 16], "dim": [3, 17, 18], "multiheaddispatchconfig": 3, "op": 4, "memory_efficient_attent": 4, "attn_bia": 4, "attentionbia": 4, "p": [4, 16], "attentionfwopbas": 4, "attentionbwopbas": 4, "doe": [4, 17], "Not": 4, "o": 4, "n": [4, 13, 16, 17], "2": [4, 12, 14, 16, 17], "format": 4, "b": [4, 17], "h": 4, "where": [4, 12, 14], "per": [4, 12, 16], "have": [4, 12, 14, 16, 18], "3": [4, 7, 14, 16, 17, 18, 20], "contigu": 4, "requir": [4, 12, 13, 17], "last": [4, 19], "s": [4, 12, 13, 14, 16, 18, 19, 20], "stride": [4, 14], "equival": [4, 11, 15, 16], "pytorch": [4, 11, 12, 15, 18, 19], "code": [4, 7, 13, 19, 20], "attn": 4, "transpos": 4, "softmax": 4, "exampl": [4, 7, 10, 12, 14, 16, 18, 19], "import": [4, 12, 14, 16, 18, 19, 20], "xop": 4, "regular": 4, "With": [4, 16], "lowertriangularmask": 4, "support": [4, 7, 16, 18, 19], "hardwar": 4, "nvidia": [4, 16, 19], "gpu": [4, 7, 17, 19], "capabl": [4, 7, 19], "abov": [4, 7, 16, 17, 18], "p100": 4, "datatyp": 4, "f16": 4, "bf16": 4, "f32": 4, "rais": 4, "notimplementederror": 4, "mha": [4, 16, 17], "valueerror": 4, "invalid": 4, "mq": 4, "mkv": 4, "kv": 4, "matrix": [4, 12], "default": [4, 7, 13, 16], "common": [4, 7], "fmha": 4, "also": [4, 7, 13, 16, 17, 19, 21], "arbitrari": 4, "slower": 4, "disabl": 4, "factor": 4, "attentionopbas": 4, "recommend": [4, 13], "best": 4, "depend": [4, 5, 20], "custom": [4, 16], "argument": [4, 13], "That": [4, 20], "function": [4, 12, 17, 18], "abil": 4, "add": 4, "qk": 4, "t": [4, 16], "calcul": 4, "n_queri": 4, "most": 4, "contain": 4, "infin": 4, "form": 4, "so": [4, 12, 13, 16, 19, 21], "some": [4, 7, 12, 14, 16, 17, 18, 19, 20], "children": 4, "defin": [4, 10, 12, 13, 16, 17, 18, 21], "altern": [4, 16, 18, 20], "thing": [4, 13], "when": [4, 7, 16, 17, 18, 19], "instead": 4, "materi": 4, "hardcod": 4, "better": [4, 12, 19], "lowertriangularmaskwithtensorbia": 4, "blockdiagonalmask": 4, "blockdiagonalcausalmask": 4, "float32": 4, "cpu": 4, "veri": [4, 16, 17, 19, 20], "slow": [4, 7], "don": [4, 16], "attempt": 4, "debug": 4, "test": [4, 10, 11, 13, 15, 16, 21], "like": [4, 13, 14, 16, 17, 18, 19, 20], "q_seqlen": 4, "k_seqlen": 4, "baseoper": 4, "cutlass": 4, "fwop": 4, "bwop": 4, "flash": 4, "triton": [4, 10, 11, 12, 15, 16], "small_k": 4, "not_supported_reason": 4, "list": [4, 10, 16], "reason": 4, "why": 4, "run": [4, 12, 16, 19], "larg": [4, 17], "includ": [4, 10, 16, 17], "without": [4, 17], "tensorcor": 4, "old": 4, "sm60": 4, "tri": 4, "dao": 4, "phil": 4, "tillet": 4, "small": [4, 12, 14], "pre": [4, 10, 14, 16], "amper": 4, "bmk": 4, "extra": [4, 12, 13, 17, 18], "reshap": 4, "might": 4, "done": [4, 13], "deprec": 4, "new": [4, 13, 16, 20, 21], "tensor_arg": 4, "tensor_kwarg": 4, "lower": [4, 12], "triangular": [4, 12], "aka": 4, "farther": 4, "initi": [4, 20], "than": [4, 7, 12, 16, 17, 19], "q_seqinfo": 4, "_seqleninfo": 4, "k_seqinfo": 4, "_batch_siz": 4, "block": [4, 6, 7, 9, 12, 13, 15, 21], "diagon": 4, "divid": 4, "handl": [4, 14], "from_tensor_list": 4, "16": [4, 12, 14, 16, 18, 20], "float16": [4, 12], "cuda": [4, 11, 12, 18, 19], "list_x": 4, "randn": [4, 12], "nn": [4, 12, 13, 17, 18, 19], "unbind": 4, "out": [4, 11, 12, 15, 16], "list_out": 4, "split": 4, "print": [4, 12, 16], "assert": 4, "from_seqlen": 4, "kv_seqlen": 4, "concaten": 4, "back": 4, "m_i": 4, "thei": [4, 7, 9, 11], "correspond": [4, 10, 19], "sum_i": 4, "invers": 4, "token": [4, 14], "possibl": [4, 10, 12, 14, 16, 19, 20], "make_causal_from_bottomright": 4, "blockdiagonalcausalfrombottomrightmask": 4, "prefix": 4, "except": 4, "nor": 4, "note": [4, 8, 12, 16, 17, 18], "num_kei": 4, "num_queri": 4, "otherwis": 4, "vector": 4, "nearer": 4, "final": 4, "blockdiagonalcausalwithoffsetpaddedkeysmask": 4, "_paddedseqleninfo": 4, "causal_diagon": 4, "offset": 4, "total": 4, "12": 4, "three": [4, 13], "max": 4, "4": [4, 14, 16], "want": [4, 7, 13, 18], "first": [4, 16, 17], "kv_pad": 4, "e": [4, 7], "area": 4, "upperbound": 4, "individu": 4, "unus": 4, "bc": 4, "memory_efficient_attention_backward": 4, "grad": 4, "lse": 4, "gradient": 4, "dq": 4, "dk": 4, "dv": 4, "explan": 4, "memory_efficient_attention_forward_requires_grad": 4, "memory_efficient_attention_forward": 4, "backward": 4, "later": 4, "positional_embed": 5, "rotaryembed": 5, "roform": 5, "su": 5, "crucial": [5, 14], "insight": 5, "rotat": 5, "matric": 5, "rel": 5, "avail": [5, 10, 19], "repo": [5, 21], "gpt": 5, "neox": 5, "inspir": [5, 21], "sinepositionalembed": 5, "positionembed": 5, "vocabembed": [5, 16], "vocab_s": [5, 10, 16], "init_weight": [5, 10], "gain": 5, "build_positional_embed": 5, "positionembeddingconfig": 5, "encod": [5, 8, 10, 11, 15], "my_position_encod": 5, "register_positional_embed": 5, "determinist": 6, "net": 6, "record_rng": 6, "set_rng": 6, "reversibleblock": [6, 17], "split_dim": 6, "f_arg": [6, 17], "g_arg": [6, 17], "backward_pass": 6, "dy": 6, "reversiblesequ": [6, 17], "modulelist": [6, 16, 17], "arg_rout": [6, 17], "transpar": 7, "sputnik": 7, "These": [7, 14, 19], "instal": 7, "recipi": 7, "machin": 7, "abl": 7, "compil": [7, 19], "git": 7, "clone": 7, "fairintern": 7, "conda": 7, "xformer_env": 7, "python": [7, 16, 19], "8": [7, 12, 16, 18], "cd": 7, "pip": 7, "txt": 7, "issu": 7, "relat": [7, 17], "nvcc": 7, "current": [7, 16, 19], "runtim": [7, 16], "match": [7, 12, 14], "often": 7, "chang": [7, 12, 16], "unload": 7, "load": 7, "xx": 7, "version": [7, 14], "gcc": 7, "re": [7, 16, 18, 20], "torch_cuda_arch_list": 7, "env": 7, "variabl": 7, "architur": 7, "suggest": 7, "setup": 7, "comprehens": 7, "export": 7, "7": [7, 14, 16], "automat": [7, 13, 21], "trigger": [7, 13], "enough": [7, 12], "less": [7, 17], "30": 7, "There": [7, 13, 16, 19, 20], "noth": 7, "specif": [7, 21], "do": [7, 13, 18, 20], "tutori": [7, 16, 19], "visibl": 7, "enabl": [7, 13, 19], "condit": 7, "met": 7, "independ": 7, "limit": [7, 12, 19], "present": [7, 14], "fullfil": 7, "block_factori": 8, "xformerencoderblock": [8, 16], "xformerencoderconfig": [8, 16, 17], "vanilla": [8, 16], "static": 8, "get_reversible_lay": 8, "input_mask": 8, "xformerdecoderblock": 8, "xformerdecoderconfig": [8, 17], "decod": [8, 10, 16], "yet": [8, 17], "target": 8, "encoder_att_mask": 8, "decoder_att_mask": 8, "complet": [9, 17, 20], "were": 9, "primarili": 9, "develop": 9, "ci": 9, "benchmark": [9, 13, 16, 19, 21], "purpos": 9, "model_factori": [10, 16, 17], "xformerconfig": [10, 14, 16], "stack_config": [10, 16], "tie_embedding_weight": 10, "weight_init": 10, "xformerweightinit": 10, "vit": [10, 11, 15], "structur": 10, "full": [10, 11, 15], "stack": [10, 16, 17], "It": [10, 13, 14, 16, 19], "share": [10, 13], "between": [10, 12, 14, 17], "improv": [10, 21], "languag": [10, 19], "press": 10, "xformer_config": [10, 14], "turn": [10, 16, 17, 20], "effect": [10, 17], "block_typ": [10, 16], "num_lay": [10, 16, 17], "residual_norm_styl": [10, 14, 16], "position_encoding_config": [10, 16], "vocab": [10, 16], "context": [10, 17], "multi_head_config": [10, 16], "res_drop": 10, "attention_mechanism_str": 10, "attn_drop": [10, 16], "feedforward_config": [10, 16], "fusedmlp": 10, "mlp_drop": 10, "gelu": 10, "mlp_multipli": [10, 14], "xformerblockconfig": 10, "serial": 10, "gener": [10, 14, 16], "easili": [10, 16, 20, 21], "bypass": 10, "use_deep_norm": 10, "src": [10, 16], "tgt": [10, 16], "encoder_input_mask": 10, "decoder_input_mask": 10, "host": [11, 15], "flexibl": [11, 16, 21], "interoper": [11, 21], "state": [11, 21], "art": [11, 21], "api": [11, 16, 20], "factori": [11, 14, 17], "replac": [11, 15, 16, 19], "exist": [11, 15, 16], "blocksparseattent": [11, 15], "extend": [11, 15, 16, 21], "zoo": [11, 15, 21], "interest": [11, 15, 16], "compar": [11, 12, 15, 17, 19, 21], "hierarch": [11, 15], "blockspars": 12, "tile": 12, "construct": [12, 13, 16], "simpl": 12, "just": [12, 14, 16, 18, 19], "minimum": 12, "coeffici": 12, "alreadi": [12, 16, 18], "mind": [12, 18], "perfect": 12, "fine": [12, 20], "after": 12, "fact": 12, "grain": 12, "still": 12, "dens": 12, "maxpool": 12, "convert": 12, "binari": 12, "layout": 12, "pleas": [12, 16, 17, 18], "now": [12, 13, 16, 18], "power": 12, "two": [12, 17, 19, 20], "let": [12, 13, 16, 18], "look": [12, 16], "seq": [12, 16, 20], "2048": [12, 16], "1024": [12, 16, 20], "block_siz": 12, "try": [12, 16], "realli": [12, 14, 16, 21], "could": [12, 14, 16], "anyth": [12, 14, 21], "causal_mask": 12, "tril": 12, "ones": 12, "causal_layout": 12, "our": [12, 18], "_you": 12, "head_": 12, "commod": 12, "multihead": [12, 20], "multi_head": [12, 20], "respons": 12, "half": 12, "fw": [12, 17], "data": [12, 20], "remov": 12, "blockif": 12, "requires_grad": 12, "particular": [12, 18], "att_val": 12, "bonu": 12, "vs": 12, "def": [12, 13, 17, 18], "mem_us": 12, "fn": 12, "titl": 12, "bookeep": 12, "start": [12, 16, 19], "empty_cach": 12, "reset_peak_memory_stat": 12, "synchron": 12, "stop": 12, "report": 12, "max_memori": 12, "max_memory_alloc": 12, "20": 12, "peak": 12, "mb": [12, 16], "round": 12, "1e6": 12, "1e3": 12, "ms": 12, "pytorch_multihead": 12, "multiheadattent": 12, "batch_first": [12, 16], "attn_mask": [12, 18], "On": 12, "v100": [12, 16, 19], "9": [12, 16], "someth": [12, 13, 16, 17, 20], "line": [12, 19], "151mb": 12, "619m": 12, "393mb": 12, "837m": 12, "more": [12, 13, 14, 16, 17, 18, 20, 21], "get": [12, 17, 18, 19], "bias": 12, "result": [12, 16, 19], "toward": 12, "privat": 13, "fork": 13, "work": [13, 18, 20], "progress": 13, "would": [13, 16, 18, 20], "point": [13, 16, 19], "directli": [13, 16, 19, 20], "order": 13, "submit": 13, "pr": [13, 21], "practic": [13, 18], "unit": [13, 16, 19], "loos": 13, "inherit": 13, "expos": [13, 16, 17, 18, 20], "exact": [13, 16], "interfac": [13, 16, 19], "consid": [13, 16, 17, 18], "dataclass": 13, "nystromselfattentionconfig": 13, "paper": [13, 17], "remark": 13, "extens": [13, 21], "explicitli": 13, "constructor": [13, 16], "field": [13, 21], "registr": 13, "snippet": 13, "ti": 13, "open": [13, 17], "up": [13, 16, 17], "least": 13, "tool": 13, "toolbox": 13, "relev": [13, 21], "pick": 13, "variant": [13, 21], "go": 13, "pytest": 13, "my_component_nam": 13, "applic": [13, 16, 17, 19], "lra": [13, 16], "json": 13, "job": [13, 16], "As": [13, 19], "remind": 13, "inform": 13, "dedic": 13, "readm": 13, "python3": [13, 16, 19], "run_task": 13, "py": [13, 16, 19], "task": 13, "config_path": 13, "world_siz": 13, "slurm": 13, "cluster": 13, "batch_submit": 13, "ck": 13, "checkpo": 13, "log": [13, 19], "process": [14, 17], "across": [14, 21], "mani": [14, 16, 19], "unchang": 14, "cross": 14, "depth": [14, 18], "prove": 14, "domain": [14, 21], "seem": 14, "howev": 14, "benefit": [14, 17], "cnn": 14, "tradeoff": [14, 17], "spatial": 14, "extent": 14, "ie": 14, "express": 14, "through": [14, 21], "patch_embed": 14, "translat": [14, 18], "anoth": [14, 17], "easier": 14, "truncat": 14, "metaform": 14, "hierarchical_config": 14, "basiclayerconfig": 14, "get_hierarchical_configur": 14, "base_hierarchical_config": 14, "attention_mechan": 14, "scaled_dot_product": 14, "patch_siz": [14, 18], "image_s": 14, "128": [14, 16], "320": 14, "256": 14, "fill": 14, "gap": 14, "walk": 16, "hierarchi": 16, "whole": 16, "mai": [16, 18], "ll": 16, "comparison": 16, "similar": [16, 19, 20], "transformerencoderlay": 16, "Its": [16, 17], "d_model": 16, "nhead": 16, "dim_feedforward": 16, "relu": [16, 19], "layer_norm_ep": 16, "1e": 16, "05": 16, "free": [16, 17], "worth": 16, "convent": 16, "forget": 16, "permut": 16, "similarli": [16, 17], "wherea": 16, "opposit": 16, "negat": 16, "fairli": 16, "close": [16, 17], "systemat": 16, "evalu": 16, "autom": 16, "prefer": 16, "put": 16, "think": 16, "declar": 16, "explicit": 16, "everyth": [16, 20], "sweep": [16, 20], "friendli": 16, "said": 16, "384": [16, 20], "encoder_config": 16, "post": 16, "whatev": 16, "encodinhg": 16, "sens": 16, "lead": 16, "lot": [16, 20], "check": [16, 18], "catch": 16, "error": 16, "earli": 16, "dummi": [16, 20], "rand": [16, 20], "ab": 16, "tranform": [16, 19], "becaus": 16, "alwai": 16, "pure": [16, 19], "ad": [16, 19], "primit": [16, 19], "am": 16, "pytorchtransform": 16, "512": 16, "num_encoder_lay": 16, "num_decoder_lay": 16, "custom_encod": 16, "exempl": 16, "below": 16, "custom_decod": 16, "appl": 16, "chosen": 16, "basi": 16, "my_config": [16, 20], "constitut": 16, "save": [16, 17], "repeat": 16, "multi_head_config_mask": 16, "multi_head_config_cross": 16, "entir": 16, "quit": 16, "few": 16, "knob": 16, "littl": [16, 20], "mostli": 16, "taken": 16, "care": 16, "speed": 16, "benchmark_pytorch_transform": 16, "loss": 16, "everi": [16, 21], "89": 16, "1182": 16, "2709": 16, "155": 16, "1950": 16, "4117": 16, "ve": 16, "replic": 16, "demonstr": 16, "wai": 16, "minim": [16, 18], "duplic": 16, "built": [16, 20], "top": 16, "advanc": 16, "doc": 16, "basic": 16, "packag": 16, "encoder_loc": 16, "encoder_random": 16, "decoder_nystrom_favor": 16, "_self_": 16, "_target_": 16, "leverag": 16, "offer": 16, "overrid": 16, "commandlin": 16, "build_model": 16, "my_model": 16, "dict_kei": 16, "pose_encod": 16, "inplac": 16, "position_embed": 16, "word_embed": 16, "resid_drop": 16, "proj": 16, "in_featur": [16, 19], "out_featur": [16, 19], "sequenti": [16, 17], "1536": 16, "wrap_att": 16, "prenorm": 16, "norm": [16, 17], "fusedlayernorm": 16, "sublay": 16, "wrap_ff": 16, "postnorm": 16, "launch": 16, "multirun": 16, "gomez": 17, "reform": 17, "unrel": 17, "lsh": 17, "chunk": 17, "lightli": 17, "adapt": 17, "robin": 17, "bruegger": 17, "lucidrain": 17, "x1": 17, "x2": 17, "produc": 17, "y1": 17, "y2": 17, "recov": 17, "detail": 17, "checkpoint": 17, "One": 17, "natur": 17, "help": 17, "commun": 17, "cost": 17, "moreov": 17, "made": 17, "increas": [17, 19], "exactli": 17, "formul": 17, "deal": 17, "accuraci": 17, "affect": 17, "verifi": 17, "repositori": [17, 18], "main": 17, "take": [17, 18, 20], "rout": 17, "compat": [17, 19], "ddp": 17, "xformerstackconfig": 17, "block_config": 17, "becom": [17, 20], "ren": 17, "urtasun": 17, "gross": 17, "2017": 17, "network": 17, "backpropag": 17, "store": 17, "kitaev": 17, "kaiser": 17, "\u0142": 17, "levskaya": 17, "sai": 18, "experi": 18, "show": 18, "how": [18, 19], "reus": [18, 21], "imag": 18, "aspect": 18, "In": [18, 19, 20], "notebook": 18, "exhaust": 18, "timm": 18, "vision_transform": 18, "visiontransform": 18, "timm_sparse_attent": 18, "timmsparseattent": 18, "img_siz": 18, "224": 18, "embed_dim": 18, "96": 18, "mlp_ratio": 18, "qkv_bia": 18, "norm_lay": 18, "layernorm": 18, "suppos": 18, "snipper": 18, "precis": 18, "sever": 18, "attention_pattern": 18, "my_fancy_mask": 18, "recurs": 18, "monkei": 18, "patch": 18, "replace_attn_with_xformers_on": 18, "module_output": 18, "isinst": 18, "qkv": 18, "child": 18, "named_children": 18, "add_modul": 18, "del": 18, "variat": 18, "exchang": 18, "good": 18, "idea": 18, "closer": 18, "exhibit": 18, "clear": 18, "sparsiti": 18, "alter": 18, "manual": 18, "sparsifi": 18, "parallel": 19, "program": 19, "backend": 19, "short": 19, "jit": 19, "toolchain": 19, "famili": 19, "consolid": 19, "hoc": 19, "goal": 19, "over": 19, "lang": 19, "org": 19, "02": 19, "html": 19, "sphx": 19, "glr": 19, "log_softmax": 19, "amp": 19, "autograd": 19, "throughput": 19, "operand": 19, "simpli": 19, "fusedlinearlay": 19, "my_linear_lay": 19, "squared_relu": 19, "skip": 19, "either": [19, 20], "septemb": 19, "faster": 19, "non": 19, "sigmoid": 19, "fp16": 19, "usecas": 19, "serv": 19, "measur": [19, 21], "laptop": 19, "3080": 19, "10": 19, "reproduc": 19, "benchmark_triton_layernorm": 19, "gb": 19, "benchmark_triton_dropout": 19, "own": 20, "requires_head_dimens": 20, "flag": 20, "defer": 20, "obscur": 20, "although": 20, "hopefulli": 20, "straightforward": 20, "intern": 20, "sure": 20, "programat": 20, "search": [20, 21], "definit": 20, "attention_nam": 20, "my": 20, "focus": 21, "agnost": 21, "design": 21, "compos": 21, "ideal": 21, "break": 21, "studi": 21, "ablat": 21, "aim": 21, "easi": 21, "focu": 21, "against": 21, "engin": 21, "effort": 21, "And": 21, "sinc": 21, "heavi": 21, "alon": 21, "happen": 21, "anytim": 21, "somebodi": 21, "crowd": 21, "welcom": 21, "move": 21, "too": 21}, "objects": {"xformers.components": [[3, 0, 1, "", "MultiHeadDispatch"], [0, 3, 0, "-", "attention"], [1, 3, 0, "-", "feedforward"], [5, 3, 0, "-", "positional_embedding"]], "xformers.components.MultiHeadDispatch": [[3, 1, 1, "", "forward"], [3, 1, 1, "", "from_config"], [3, 2, 1, "", "training"]], "xformers.components.attention": [[0, 0, 1, "", "Attention"], [0, 0, 1, "", "AttentionMask"], [0, 0, 1, "", "FavorAttention"], [0, 0, 1, "", "GlobalAttention"], [0, 0, 1, "", "LinformerAttention"], [0, 0, 1, "", "LocalAttention"], [0, 0, 1, "", "NystromAttention"], [0, 0, 1, "", "OrthoFormerAttention"], [0, 0, 1, "", "RandomAttention"], [0, 0, 1, "", "ScaledDotProduct"], [0, 5, 1, "", "build_attention"], [0, 5, 1, "", "register_attention"]], "xformers.components.attention.Attention": [[0, 1, 1, "", "forward"], [0, 1, 1, "", "from_config"]], "xformers.components.attention.AttentionMask": [[0, 4, 1, "", "device"], [0, 4, 1, "", "dtype"], [0, 1, 1, "", "from_bool"], [0, 1, 1, "", "from_multiplicative"], [0, 4, 1, "", "is_sparse"], [0, 1, 1, "", "make_causal"], [0, 1, 1, "", "make_crop"], [0, 4, 1, "", "ndim"], [0, 4, 1, "", "shape"], [0, 1, 1, "", "to"], [0, 1, 1, "", "to_bool"]], "xformers.components.attention.FavorAttention": [[0, 1, 1, "", "__init__"], [0, 1, 1, "", "forward"]], "xformers.components.attention.GlobalAttention": [[0, 1, 1, "", "__init__"], [0, 1, 1, "", "forward"]], "xformers.components.attention.LinformerAttention": [[0, 1, 1, "", "__init__"], [0, 1, 1, "", "forward"]], "xformers.components.attention.LocalAttention": [[0, 1, 1, "", "__init__"], [0, 1, 1, "", "forward"]], "xformers.components.attention.NystromAttention": [[0, 1, 1, "", "__init__"], [0, 1, 1, "", "forward"]], "xformers.components.attention.OrthoFormerAttention": [[0, 1, 1, "", "__init__"], [0, 1, 1, "", "forward"]], "xformers.components.attention.RandomAttention": [[0, 1, 1, "", "__init__"], [0, 1, 1, "", "forward"]], "xformers.components.attention.ScaledDotProduct": [[0, 1, 1, "", "forward"], [0, 2, 1, "", "mask"]], "xformers.components.feedforward": [[1, 0, 1, "", "Feedforward"], [1, 0, 1, "", "MLP"], [1, 5, 1, "", "build_feedforward"], [1, 5, 1, "", "register_feedforward"]], "xformers.components.feedforward.Feedforward": [[1, 1, 1, "", "from_config"], [1, 2, 1, "", "training"]], "xformers.components.feedforward.MLP": [[1, 1, 1, "", "forward"], [1, 2, 1, "", "training"]], "xformers.components.positional_embedding": [[5, 0, 1, "", "RotaryEmbedding"], [5, 0, 1, "", "SinePositionalEmbedding"], [5, 0, 1, "", "VocabEmbedding"], [5, 5, 1, "", "build_positional_embedding"], [5, 5, 1, "", "register_positional_embedding"]], "xformers.components.positional_embedding.RotaryEmbedding": [[5, 1, 1, "", "forward"], [5, 2, 1, "", "training"]], "xformers.components.positional_embedding.SinePositionalEmbedding": [[5, 1, 1, "", "forward"], [5, 2, 1, "", "training"]], "xformers.components.positional_embedding.VocabEmbedding": [[5, 1, 1, "", "forward"], [5, 1, 1, "", "init_weights"], [5, 2, 1, "", "training"]], "xformers.components.reversible": [[6, 0, 1, "", "Deterministic"], [6, 0, 1, "", "ReversibleBlock"], [6, 0, 1, "", "ReversibleSequence"]], "xformers.components.reversible.Deterministic": [[6, 1, 1, "", "forward"], [6, 1, 1, "", "record_rng"], [6, 2, 1, "", "training"]], "xformers.components.reversible.ReversibleBlock": [[6, 1, 1, "", "backward_pass"], [6, 1, 1, "", "forward"], [6, 2, 1, "", "training"]], "xformers.components.reversible.ReversibleSequence": [[6, 1, 1, "", "forward"], [6, 2, 1, "", "training"]], "xformers.factory": [[8, 3, 0, "-", "block_factory"], [10, 3, 0, "-", "model_factory"]], "xformers.factory.block_factory": [[8, 0, 1, "", "xFormerDecoderBlock"], [8, 0, 1, "", "xFormerEncoderBlock"]], "xformers.factory.block_factory.xFormerDecoderBlock": [[8, 1, 1, "", "forward"], [8, 1, 1, "", "from_config"], [8, 2, 1, "", "training"]], "xformers.factory.block_factory.xFormerEncoderBlock": [[8, 1, 1, "", "forward"], [8, 1, 1, "", "from_config"], [8, 1, 1, "", "get_reversible_layer"], [8, 2, 1, "", "training"]], "xformers.factory.model_factory": [[10, 0, 1, "", "xFormer"], [10, 0, 1, "", "xFormerConfig"]], "xformers.factory.model_factory.xFormer": [[10, 1, 1, "", "__init__"], [10, 1, 1, "", "forward"], [10, 1, 1, "", "from_config"], [10, 1, 1, "", "init_weights"], [10, 2, 1, "", "training"]], "xformers.factory.model_factory.xFormerConfig": [[10, 2, 1, "", "stack_configs"], [10, 2, 1, "", "tie_embedding_weights"], [10, 2, 1, "", "weight_init"]], "xformers": [[4, 3, 0, "-", "ops"], [7, 3, 0, "-", "triton"]], "xformers.ops": [[4, 0, 1, "", "AttentionBias"], [4, 0, 1, "", "AttentionOpBase"], [4, 3, 0, "-", "fmha"], [4, 5, 1, "", "memory_efficient_attention"]], "xformers.ops.AttentionBias": [[4, 1, 1, "", "materialize"]], "xformers.ops.AttentionOpBase": [[4, 1, 1, "", "not_supported_reasons"]], "xformers.ops.fmha": [[4, 3, 0, "-", "attn_bias"], [4, 3, 0, "-", "cutlass"], [4, 3, 0, "-", "flash"], [4, 5, 1, "", "memory_efficient_attention_backward"], [4, 5, 1, "", "memory_efficient_attention_forward"], [4, 5, 1, "", "memory_efficient_attention_forward_requires_grad"], [4, 3, 0, "-", "small_k"], [4, 3, 0, "-", "triton"]], "xformers.ops.fmha.attn_bias": [[4, 0, 1, "", "AttentionBias"], [4, 0, 1, "", "BlockDiagonalCausalFromBottomRightMask"], [4, 0, 1, "", "BlockDiagonalCausalMask"], [4, 0, 1, "", "BlockDiagonalCausalWithOffsetPaddedKeysMask"], [4, 0, 1, "", "BlockDiagonalMask"], [4, 0, 1, "", "LowerTriangularMask"], [4, 0, 1, "", "LowerTriangularMaskWithTensorBias"]], "xformers.ops.fmha.attn_bias.AttentionBias": [[4, 1, 1, "", "materialize"]], "xformers.ops.fmha.attn_bias.BlockDiagonalCausalWithOffsetPaddedKeysMask": [[4, 1, 1, "", "from_seqlens"], [4, 1, 1, "", "materialize"]], "xformers.ops.fmha.attn_bias.BlockDiagonalMask": [[4, 1, 1, "", "from_seqlens"], [4, 1, 1, "", "from_tensor_list"], [4, 1, 1, "", "make_causal"], [4, 1, 1, "", "make_causal_from_bottomright"], [4, 1, 1, "", "materialize"], [4, 1, 1, "", "split"]], "xformers.ops.fmha.cutlass": [[4, 0, 1, "", "BwOp"], [4, 0, 1, "", "FwOp"]], "xformers.ops.fmha.flash": [[4, 0, 1, "", "BwOp"], [4, 0, 1, "", "FwOp"]], "xformers.ops.fmha.small_k": [[4, 0, 1, "", "BwOp"], [4, 0, 1, "", "FwOp"]], "xformers.ops.fmha.triton": [[4, 0, 1, "", "BwOp"], [4, 0, 1, "", "FwOp"]]}, "objtypes": {"0": "py:class", "1": "py:method", "2": "py:attribute", "3": "py:module", "4": "py:property", "5": "py:function"}, "objnames": {"0": ["py", "class", "Python class"], "1": ["py", "method", "Python method"], "2": ["py", "attribute", "Python attribute"], "3": ["py", "module", "Python module"], "4": ["py", "property", "Python property"], "5": ["py", "function", "Python function"]}, "titleterms": {"attent": [0, 3, 4, 18, 20], "mechan": [0, 1, 20], "feedforward": 1, "api": 2, "refer": [2, 7], "multi": 3, "head": 3, "xformer": [4, 11, 13, 16, 21], "optim": 4, "oper": 4, "memori": 4, "effici": 4, "avail": 4, "implement": 4, "bias": 4, "non": 4, "autograd": 4, "posit": 5, "embed": 5, "revers": [6, 17], "layer": [6, 16, 19], "custom": [7, 11], "part": [7, 11, 13], "spars": [7, 18], "cuda": 7, "kernel": 7, "1": 7, "build": [7, 11, 16], "2": 7, "usag": 7, "triton": [7, 19], "requir": 7, "possibl": 7, "block": [8, 11, 16, 17], "factori": [8, 9, 10, 16], "model": [10, 11, 16, 18], "welcom": 11, "s": 11, "document": 11, "compon": 11, "programat": 11, "tutori": [11, 15], "exampl": 11, "some": 11, "us": [12, 17, 19], "blocksparseattent": 12, "extend": 13, "zoo": 13, "hierarch": 14, "transform": [14, 16, 17], "an": [16, 18], "encod": 16, "compar": 16, "pytorch": 16, "warn": 16, "full": 16, "hydra": 16, "intro": 17, "In": 17, "practic": 17, "replac": 18, "all": 18, "from": 18, "exist": 18, "vit": 18, "equival": 18, "base": 19, "fuse": 19, "softmax": 19, "linear": 19, "norm": 19, "dropout": 19, "bia": 19, "activ": 19, "i": 20, "m": 20, "onli": 20, "interest": 20, "test": 20, "out": 20, "ar": 20, "host": 20, "here": 20, "what": 21}, "envversion": {"sphinx.domains.c": 2, "sphinx.domains.changeset": 1, "sphinx.domains.citation": 1, "sphinx.domains.cpp": 6, "sphinx.domains.index": 1, "sphinx.domains.javascript": 2, "sphinx.domains.math": 2, "sphinx.domains.python": 3, "sphinx.domains.rst": 2, "sphinx.domains.std": 2, "sphinx.ext.intersphinx": 1, "sphinx.ext.todo": 2, "sphinx.ext.viewcode": 1, "sphinx": 56}})